---
title: "k8s-ì‚½ì§ˆ-containerd"
date: 2025-11-26T00:26:39+09:00
draft: false
categories: ["ì‚½ì§ˆ","k8s","containerd"]
tags: ["ì‚½ì§ˆ","k8s","containerd","ë§ˆì´ê·¸ë ˆì´ì…˜"]
description: "k8s-ì‚½ì§ˆ-containerd"
author: "ëŠ¦ì°Œë¯¼"
---

# Kubernetes VM ë¡¤ë°± í›„ í´ëŸ¬ìŠ¤í„° ë³µêµ¬ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

> Worker ë…¸ë“œ VM ë¡¤ë°± í›„ ë°œìƒí•œ ë‹¤ì–‘í•œ ì¥ì• ë¥¼ í•´ê²°í•˜ëŠ” ê³¼ì •ì„ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.

## ğŸ“‹ ì‚¬ê±´ ê°œìš”

- **í™˜ê²½**: Kubernetes v1.31.13 (1 Control Plane + 2 Worker)
- **CNI**: Cilium
- **ë°œìƒ ì›ì¸**: Worker1 VMì„ ì´ì „ ìŠ¤ëƒ…ìƒ·ìœ¼ë¡œ ë¡¤ë°±
- **í•µì‹¬ ë¬¸ì œ**: **containerd ë²„ì „ ë¶ˆì¼ì¹˜**

---

## ğŸ¯ ì „ì²´ íƒ€ì„ë¼ì¸

```

[ì‚¬ê±´ ë°œìƒ]
Worker1 VM ë¡¤ë°± (ìŠ¤ëƒ…ìƒ· ë³µì›)
        â†“
[ë¬¸ì œ 1] hostname ë¶ˆì¼ì¹˜
        â†“
[ë¬¸ì œ 2] API ì„œë²„ ì£¼ì†Œ ë¶ˆì¼ì¹˜
        â†“
[ë¬¸ì œ 3] br_netfilter ëª¨ë“ˆ ëˆ„ë½
        â†“
[ë¬¸ì œ 4] Kubernetes ë²„ì „ ë¶ˆì¼ì¹˜
        â†“
[ë¬¸ì œ 5] Control Plane ì‹¤ìˆ˜ë¡œ ë¦¬ì…‹
        â†“
[í´ëŸ¬ìŠ¤í„° ì¬êµ¬ì„±]
        â†“
[ë¬¸ì œ 6] kubelet ìë™ ì‹œì‘ ì•ˆë¨
        â†“
[ë¬¸ì œ 7] CNI Pod CrashLoopBackOff â† containerd ë²„ì „!
        â†“
[í•´ê²°] containerd 2.1.5ë¡œ í†µì¼

```

---

## ğŸ”¥ ë¬¸ì œ 1: hostname ë¶ˆì¼ì¹˜

### ì¦ìƒ

```bash
$ kubectl get nodes
NAME          STATUS     ROLES           AGE   VERSION
k8s-worker1   NotReady   <none>          23d   v1.31.13  # ê¸°ì¡´
w1            NotReady   <none>          51s   v1.29.15  # ìŠ¤ëƒ…ìƒ·ì—ì„œ ë³µì› (ì¤‘ë³µ!)

```

### ì›ì¸
ìŠ¤ëƒ…ìƒ· ì‹œì ì˜ hostnameì´ `w1`ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆì—ˆìŒ

```bash
# Worker1ì—ì„œ í™•ì¸
$ hostname
w1  # ì˜ˆìƒ: k8s-worker1

```

### í•´ê²°

```bash
# Worker1ì—ì„œ ì‹¤í–‰
sudo hostnamectl set-hostname k8s-worker1

# Control Planeì—ì„œ ì¤‘ë³µ ë…¸ë“œ ì‚­ì œ
kubectl delete node w1
kubectl delete node k8s-worker1

```

---

## ğŸ”¥ ë¬¸ì œ 2: API ì„œë²„ ì£¼ì†Œ ë¶ˆì¼ì¹˜

### ì¦ìƒ

```bash
# Worker1 kubelet ë¡œê·¸
$ journalctl -u kubelet | tail -10
dial tcp 10.0.0.50:6443: connect: no route to host

```

### ì›ì¸
ìŠ¤ëƒ…ìƒ· ì‹œì ì˜ `/etc/kubernetes/kubelet.conf`ì— ê¸°ë¡ëœ API ì„œë²„ ì£¼ì†Œê°€ í˜„ì¬ì™€ ë‹¤ë¦„

```yaml
# /etc/kubernetes/kubelet.conf
clusters:
- cluster:
    server: https://10.0.0.50:6443  # êµ¬ Control Plane IP

```

í˜„ì¬ Control Plane: `https://10.0.0.100:6443`

### í•´ê²°
Worker ë…¸ë“œ ì¬ì¡°ì¸ í•„ìš”:

```bash
# Control Planeì—ì„œ í† í° ìƒì„±
kubeadm token create --print-join-command

# Worker1ì—ì„œ ì¬ì¡°ì¸
sudo kubeadm reset -f
sudo rm -rf /etc/cni/net.d/*
sudo kubeadm join 10.0.0.100:6443 --token <token> --discovery-token-ca-cert-hash <hash>

```

---

## ğŸ”¥ ë¬¸ì œ 3: br_netfilter ëª¨ë“ˆ ëˆ„ë½

### ì¦ìƒ

```bash
$ sudo kubeadm join ...
[ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]:
/proc/sys/net/bridge/bridge-nf-call-iptables does not exist

```

### ì›ì¸
ìŠ¤ëƒ…ìƒ· ë³µì› ì‹œ ì»¤ë„ ëª¨ë“ˆì´ ìë™ ë¡œë“œë˜ì§€ ì•ŠìŒ

### í•´ê²°

```bash
# ëª¨ë“ˆ ë¡œë“œ
sudo modprobe br_netfilter

# ì»¤ë„ íŒŒë¼ë¯¸í„° ì„¤ì •
sudo sysctl -w net.bridge.bridge-nf-call-iptables=1

# ì˜êµ¬ ì„¤ì •
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
EOF

sudo sysctl --system

```

---

## ğŸ”¥ ë¬¸ì œ 4: Kubernetes ë²„ì „ ë¶ˆì¼ì¹˜

### ì¦ìƒ

```bash
$ kubectl get nodes
NAME          STATUS   VERSION
k8s-cp        Ready    v1.31.13
k8s-worker1   Ready    v1.29.15  # ë²„ì „ ë‹¤ë¦„!
k8s-worker2   Ready    v1.31.13

```

### í•´ê²°
Worker1 ì—…ê·¸ë ˆì´ë“œ:

```bash
# Worker1ì—ì„œ ì‹¤í–‰
# 1. íŒ¨í‚¤ì§€ ì €ì¥ì†Œ ì—…ë°ì´íŠ¸
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | \
  sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg --yes

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | \
  sudo tee /etc/apt/sources.list.d/kubernetes.list

# 2. ì—…ê·¸ë ˆì´ë“œ
sudo apt-get update
sudo apt-get install -y kubeadm=1.31.13-1.1 kubelet=1.31.13-1.1 kubectl=1.31.13-1.1

# 3. ë…¸ë“œ ì—…ê·¸ë ˆì´ë“œ
sudo kubeadm upgrade node

# 4. kubelet ì¬ì‹œì‘
sudo systemctl daemon-reload
sudo systemctl restart kubelet

```

---

## ğŸ”¥ ë¬¸ì œ 5: Control Plane ì‹¤ìˆ˜ë¡œ ë¦¬ì…‹

### ì‚¬ê±´
Workerìš© ëª…ë ¹ì–´ë¥¼ Control Planeì—ì„œ ì‹¤í–‰:

```bash
# CPì—ì„œ ì‹¤ìˆ˜ë¡œ ì‹¤í–‰...
sudo kubeadm reset -f

```

### ê²°ê³¼

```bash
$ kubectl get nodes
The connection to the server 10.0.0.100:6443 was refused

```

etcd ë°ì´í„° ì†ì‹¤, í´ëŸ¬ìŠ¤í„° ì™„ì „ ì†ìƒ

### ë³µêµ¬
Control Plane ì¬ì´ˆê¸°í™”:

```bash
# 1. ì™„ì „ ì •ë¦¬
sudo pkill -9 kubelet
sudo rm -rf /etc/kubernetes/manifests/*
sudo rm -rf /var/lib/etcd/*
sudo rm -rf /etc/cni/net.d/*

# 2. ì¬ì´ˆê¸°í™”
sudo kubeadm init --pod-network-cidr=10.244.0.0/16

# 3. kubeconfig ì„¤ì •
mkdir -p $HOME/.kube
sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# 4. CNI ì¬ì„¤ì¹˜
cilium install

# 5. Worker ë…¸ë“œ ì¬ì¡°ì¸

```

---

## ğŸ”¥ ë¬¸ì œ 6: kubelet ìë™ ì‹œì‘ ì•ˆë¨

### ì¦ìƒ
ì¬ë¶€íŒ… í›„:

```bash
$ kubectl get nodes
The connection to the server 10.0.0.100:6443 was refused

$ sudo systemctl status kubelet
â— kubelet.service
     Active: inactive (dead)

```

### ì›ì¸
`kubeadm reset` ì´í›„ kubeletì´ disabled ìƒíƒœë¡œ ë³€ê²½ë¨

### í•´ê²°

```bash
sudo systemctl start kubelet
sudo systemctl enable kubelet  # ì¬ë¶€íŒ… ì‹œ ìë™ ì‹œì‘

```

---

## ğŸ”¥ ë¬¸ì œ 7: CNI Pod CrashLoopBackOff (í•µì‹¬ ë¬¸ì œ!)

### ì¦ìƒ

```bash
$ kubectl get pods -n kube-system
NAME                    READY   STATUS             RESTARTS
cilium-abc123          0/1     CrashLoopBackOff   5
cilium-envoy-xyz789    0/1     CrashLoopBackOff   5

$ kubectl get nodes
NAME          STATUS     ROLES           AGE   VERSION
k8s-cp        Ready      control-plane   1h    v1.31.13
k8s-worker1   NotReady   <none>          30m   v1.31.13  # CNI ì—†ì–´ì„œ NotReady
k8s-worker2   Ready      <none>          1h    v1.31.13

```

### ì›ì¸ ë¶„ì„

#### 1ë‹¨ê³„: Pod ë¡œê·¸ í™•ì¸

```bash
$ kubectl logs -n kube-system cilium-abc123
Error: failed to create containerd client: rpc error: code = Unavailable desc = connection error

```

#### 2ë‹¨ê³„: containerd ë²„ì „ í™•ì¸

```bash
# Control Plane
$ containerd --version
containerd containerd.io v2.1.5

# Worker1 (ë¬¸ì œì˜ ë…¸ë“œ)
$ containerd --version
containerd containerd.io v1.6.28  # ë²„ì „ ë‹¤ë¦„!

# Worker2
$ containerd --version
containerd containerd.io v2.1.5

```

**ë°œê²¬**: Worker1ì˜ containerd ë²„ì „ì´ ë‚®ìŒ (ìŠ¤ëƒ…ìƒ· ë³µì› ë•Œë¬¸)

### í•´ê²°: containerd ì—…ê·¸ë ˆì´ë“œ

```bash
# Worker1ì—ì„œ ì‹¤í–‰

# 1. ê¸°ì¡´ containerd ì œê±°
sudo systemctl stop containerd
sudo apt-get remove -y containerd.io

# 2. ìµœì‹  ë²„ì „ ì„¤ì¹˜
sudo apt-get update
sudo apt-get install -y containerd.io=1.7.24-1

# ë˜ëŠ” íŠ¹ì • ë²„ì „ ì„¤ì¹˜ (2.1.5)
curl -LO https://github.com/containerd/containerd/releases/download/v2.1.5/containerd-2.1.5-linux-amd64.tar.gz
sudo tar Cxzvf /usr/local containerd-2.1.5-linux-amd64.tar.gz

# 3. ì„¤ì • ì ìš©
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml

# SystemdCgroup í™œì„±í™” (í•„ìˆ˜!)
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

# 4. ì¬ì‹œì‘
sudo systemctl restart containerd
sudo systemctl restart kubelet

# 5. í™•ì¸
containerd --version

```

### ê²°ê³¼

```bash
$ kubectl get nodes
NAME          STATUS   ROLES           AGE   VERSION
k8s-cp        Ready    control-plane   5h    v1.31.13
k8s-worker1   Ready    <none>          4h    v1.31.13  # Ready!
k8s-worker2   Ready    <none>          5h    v1.31.13

$ kubectl get pods -n kube-system | grep cilium
cilium-abc123          1/1     Running   0       2m
cilium-envoy-xyz789    1/1     Running   0       2m
cilium-operator-...    1/1     Running   0       2m

```

---

## ğŸ“Š CNI/CSI íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ë¡œê·¸ í™•ì¸ ê°€ì´ë“œ

### CNI ë¬¸ì œ ì§„ë‹¨

#### 1. ë…¸ë“œ ìƒíƒœ í™•ì¸

```bash
$ kubectl get nodes
NAME          STATUS     ROLES    AGE   VERSION
worker1       NotReady   <none>   10m   v1.31.13

$ kubectl describe node worker1 | grep -A 10 "Conditions:"
Ready   False   KubeletNotReady
        container runtime network not ready: NetworkReady=false
        reason:NetworkPluginNotReady
        message:Network plugin returns error: cni plugin not initialized

```

#### 2. CNI Pod ìƒíƒœ í™•ì¸

```bash
# CNI Pod ì°¾ê¸°
$ kubectl get pods -n kube-system -o wide | grep -E "cilium|calico|flannel"
cilium-abc123   0/1   CrashLoopBackOff   5   10m   worker1

# Pod ë¡œê·¸ í™•ì¸
$ kubectl logs -n kube-system cilium-abc123

# ì´ì „ ë¡œê·¸ í™•ì¸ (ì¬ì‹œì‘ ë°˜ë³µ ì‹œ)
$ kubectl logs -n kube-system cilium-abc123 --previous

```

#### 3. CNI ì„¤ì • íŒŒì¼ í™•ì¸

```bash
# Worker ë…¸ë“œì—ì„œ
$ ls -la /etc/cni/net.d/
05-cilium.conflist

$ cat /etc/cni/net.d/05-cilium.conflist
# ì„¤ì • ë‚´ìš© í™•ì¸

```

#### 4. kubelet ë¡œê·¸ í™•ì¸

```bash
# Worker ë…¸ë“œì—ì„œ
$ sudo journalctl -u kubelet --no-pager | tail -50
$ sudo journalctl -u kubelet -f  # ì‹¤ì‹œê°„ ë¡œê·¸

```

### CSI ë¬¸ì œ ì§„ë‹¨

#### 1. PVC ìƒíƒœ í™•ì¸

```bash
$ kubectl get pvc
NAME      STATUS    VOLUME   CAPACITY   STORAGECLASS
my-pvc    Pending                        longhorn

$ kubectl describe pvc my-pvc
Events:
  Type     Reason                Age   From                         Message
  ----     ------                ----  ----                         -------
  Warning  ProvisioningFailed    5s    persistentvolume-controller
           Failed to provision volume: waiting for a volume to be created

```

#### 2. CSI Controller í™•ì¸

```bash
# CSI Controller Pod ì°¾ê¸°
$ kubectl get pods -n longhorn-system | grep controller
longhorn-csi-controller-...   3/3   Running

# ë¡œê·¸ í™•ì¸ (ê° ì»¨í…Œì´ë„ˆë³„ë¡œ)
$ kubectl logs -n longhorn-system longhorn-csi-controller-... -c csi-provisioner
$ kubectl logs -n longhorn-system longhorn-csi-controller-... -c csi-attacher
$ kubectl logs -n longhorn-system longhorn-csi-controller-... -c longhorn-manager

```

#### 3. CSI Node Plugin í™•ì¸

```bash
# íŠ¹ì • ë…¸ë“œì˜ CSI Node Plugin ì°¾ê¸°
$ kubectl get pods -n longhorn-system -o wide | grep node | grep worker1
longhorn-csi-plugin-abc   3/3   Running   0   10m   worker1

# ë¡œê·¸ í™•ì¸
$ kubectl logs -n longhorn-system longhorn-csi-plugin-abc -c node-driver-registrar
$ kubectl logs -n longhorn-system longhorn-csi-plugin-abc -c longhorn-csi-plugin

```

#### 4. StorageClass í™•ì¸

```bash
$ kubectl get storageclass
NAME                 PROVISIONER          RECLAIMPOLICY
longhorn (default)   driver.longhorn.io   Delete

$ kubectl describe storageclass longhorn

```

#### 5. VolumeAttachment í™•ì¸

```bash
# Podê°€ ì–´ëŠ ë…¸ë“œì— ìˆëŠ”ì§€ í™•ì¸
$ kubectl get pods -o wide
my-pod   Running   worker1

# VolumeAttachment í™•ì¸
$ kubectl get volumeattachment
NAME                                                                   ATTACHED   AGE
csi-xyz123...   true       5m

$ kubectl describe volumeattachment csi-xyz123...

```

### Container Runtime ë¬¸ì œ ì§„ë‹¨

#### 1. containerd ìƒíƒœ í™•ì¸

```bash
# Worker ë…¸ë“œì—ì„œ
$ sudo systemctl status containerd
$ sudo journalctl -u containerd --no-pager | tail -50

```

#### 2. containerd ë²„ì „ í™•ì¸

```bash
$ containerd --version
containerd containerd.io v2.1.5

# CRI ë²„ì „ í™•ì¸
$ sudo crictl version

```

#### 3. ì»¨í…Œì´ë„ˆ ëª©ë¡ í™•ì¸

```bash
# ì‹¤í–‰ ì¤‘ì¸ ì»¨í…Œì´ë„ˆ
$ sudo crictl ps

# ëª¨ë“  ì»¨í…Œì´ë„ˆ (ì¤‘ì§€ëœ ê²ƒ í¬í•¨)
$ sudo crictl ps -a

# Pod ëª©ë¡
$ sudo crictl pods

```

#### 4. ì´ë¯¸ì§€ í™•ì¸

```bash
# ì´ë¯¸ì§€ ëª©ë¡
$ sudo crictl images

# íŠ¹ì • ì´ë¯¸ì§€ pull í…ŒìŠ¤íŠ¸
$ sudo crictl pull quay.io/cilium/cilium:v1.18.2

```

---

## ğŸ¯ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì²´í¬ë¦¬ìŠ¤íŠ¸

VM ë¡¤ë°± í›„ Worker ë…¸ë“œ ì¬ê°€ì… ì‹œ:

```

â–¡ 1. hostname í™•ì¸ ë° ìˆ˜ì •
   - hostname
   - /etc/hostname

â–¡ 2. ì»¤ë„ ëª¨ë“ˆ í™•ì¸
   - lsmod | grep br_netfilter
   - /etc/modules-load.d/k8s.conf

â–¡ 3. ì»¤ë„ íŒŒë¼ë¯¸í„° í™•ì¸
   - sysctl net.bridge.bridge-nf-call-iptables
   - /etc/sysctl.d/k8s.conf

â–¡ 4. containerd ë²„ì „ í™•ì¸ â† ì¤‘ìš”!
   - containerd --version
   - ë‹¤ë¥¸ ë…¸ë“œì™€ ë²„ì „ ì¼ì¹˜ í™•ì¸

â–¡ 5. Kubernetes ë²„ì „ í™•ì¸
   - kubelet --version
   - Control Planeê³¼ ë²„ì „ í˜¸í™˜ì„± í™•ì¸

â–¡ 6. kubelet ì„¤ì •
   - systemctl status kubelet
   - systemctl enable kubelet

â–¡ 7. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
   - ping Control Plane IP
   - telnet Control Plane IP 6443

â–¡ 8. ê¸°ì¡´ Kubernetes ì„¤ì • ì œê±°
   - kubeadm reset -f
   - rm -rf /etc/cni/net.d/*
   - rm -rf /var/lib/kubelet/*

â–¡ 9. í´ëŸ¬ìŠ¤í„° ì¬ì¡°ì¸
   - kubeadm join ...

â–¡ 10. CNI Pod ìƒíƒœ í™•ì¸
   - kubectl get pods -n kube-system

```

---

## ğŸ“ êµí›ˆ ë° ì˜ˆë°©ì±…

### 1. VM ìŠ¤ëƒ…ìƒ· ê´€ë¦¬
- **ìŠ¤ëƒ…ìƒ· ì´ë¦„ì— ë‚ ì§œ/ë²„ì „ ëª…ì‹œ**: `worker1-k8s-v1.31-containerd-2.1.5-20250115`
- **ë³µì› ì „ ë²„ì „ ì •ë³´ í™•ì¸**: í˜„ì¬ í´ëŸ¬ìŠ¤í„°ì™€ í˜¸í™˜ë˜ëŠ”ì§€ ì²´í¬
- **ë³µì› í›„ ì¦‰ì‹œ í™•ì¸**:
  ```bash
  hostname
  containerd --version
  kubelet --version
  ```

### 2. ë²„ì „ í†µì¼ ê´€ë¦¬
ëª¨ë“  ë…¸ë“œì—ì„œ ë™ì¼í•œ ë²„ì „ ì‚¬ìš©:

```bash
# ë²„ì „ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
#!/bin/bash
for node in cp worker1 worker2; do
  echo "=== $node ==="
  ssh $node "hostname && containerd --version && kubelet --version"
done

```

### 3. ìë™í™”ëœ ë…¸ë“œ ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# k8s-node-prepare.sh

# 1. Hostname ì„¤ì •
if [ -z "$1" ]; then
  echo "Usage: $0 <hostname>"
  exit 1
fi

sudo hostnamectl set-hostname $1

# 2. ì»¤ë„ ëª¨ë“ˆ
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
overlay
EOF

sudo modprobe br_netfilter
sudo modprobe overlay

# 3. ì»¤ë„ íŒŒë¼ë¯¸í„°
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
EOF

sudo sysctl --system

# 4. containerd ì„¤ì •
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

# 5. kubelet enable
sudo systemctl enable kubelet
sudo systemctl enable containerd

echo "âœ… Node preparation complete for: $1"

```

### 4. ì •ê¸° ë°±ì—…
- **etcd ë°±ì—…** (Control Plane):
  ```bash
  ETCDCTL_API=3 etcdctl snapshot save /backup/etcd-$(date +%Y%m%d).db \
    --endpoints=https://127.0.0.1:2379 \
    --cacert=/etc/kubernetes/pki/etcd/ca.crt \
    --cert=/etc/kubernetes/pki/etcd/server.crt \
    --key=/etc/kubernetes/pki/etcd/server.key
  ```

- **Kubernetes ë¦¬ì†ŒìŠ¤ ë°±ì—…**:
  ```bash
  kubectl get all -A -o yaml > k8s-resources-$(date +%Y%m%d).yaml
  ```

### 5. ëª¨ë‹ˆí„°ë§ ì„¤ì •
ë…¸ë“œ ìƒíƒœ ìë™ ì•Œë¦¼:

```bash
# Slack/Discord webhookìœ¼ë¡œ ì•Œë¦¼
while true; do
  NOT_READY=$(kubectl get nodes | grep NotReady | wc -l)
  if [ $NOT_READY -gt 0 ]; then
    curl -X POST $WEBHOOK_URL -d '{"text":"âš ï¸ K8s Node NotReady detected!"}'
  fi
  sleep 60
done

```

---

## ğŸ” containerd ë²„ì „ í˜¸í™˜ì„±

| Kubernetes | containerd ê¶Œì¥ ë²„ì „ |
|------------|---------------------|
| v1.31.x    | v1.7.x, v2.1.x      |
| v1.30.x    | v1.7.x, v2.0.x      |
| v1.29.x    | v1.6.x, v1.7.x      |

**ì¤‘ìš”**: í´ëŸ¬ìŠ¤í„° ë‚´ ëª¨ë“  ë…¸ë“œëŠ” **ë™ì¼í•œ containerd ë²„ì „** ì‚¬ìš© ê¶Œì¥!

---

## ìµœì¢… ìƒíƒœ

```bash
$ kubectl get nodes -o wide
NAME          STATUS   ROLES           VERSION    CONTAINER-RUNTIME
k8s-cp        Ready    control-plane   v1.31.13   containerd://2.1.5
k8s-worker1   Ready    <none>          v1.31.13   containerd://2.1.5
k8s-worker2   Ready    <none>          v1.31.13   containerd://2.1.5

$ kubectl get pods -n kube-system | grep cilium
cilium-...          1/1     Running   0       30m
cilium-envoy-...    1/1     Running   0       30m
cilium-operator-... 1/1     Running   0       30m

```

ëª¨ë“  ë…¸ë“œ ì •ìƒ ë™ì‘! ğŸ‰

---

## ì°¸ê³  ìë£Œ

- [containerd Release Notes](https://github.com/containerd/containerd/releases)
- [Kubernetes Component Version Skew](https://kubernetes.io/releases/version-skew-policy/)
- [Cilium Troubleshooting Guide](https://docs.cilium.io/en/stable/operations/troubleshooting/)
- [kubeadm Troubleshooting](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/)


